% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/GompPois_flatNIG_mcmc.R
\name{GompPois_flatNIG_mcmc}
\alias{GompPois_flatNIG_mcmc}
\title{Gompertz Model with Poisson Sampling Error under Flat and
Normal-Inverse Gamma Prior: Markov Chain Monte Carlo Algorithm}
\usage{
GompPois_flatNIG_mcmc = function(nsim, Nstar, phi1, phi2, eta1, eta2,
                                 starter = NULL, burn = 1, thin = 1,
                                 verbose = +Inf)
}
\arguments{
\item{nsim}{The number of complete scans.}

\item{Nstar}{The vector of the data.}

\item{phi1}{Shape parameter of inverse gamma in NIG prior.}

\item{phi2}{Rate parameter of inverse gamma in NIG prior.}

\item{eta1}{Mean parameter of normal in NIG prior.}

\item{eta2}{Scale parameter of normal in NIG prior.}

\item{starter}{List for starting point.}

\item{burn}{The number of draws to be discarded as burn-in.}

\item{thin}{The thinning parameter.}

\item{verbose}{The period for printing status of the chain.}
}
\value{
Posterior sample of parameters.
}
\description{
Posterior simulation using elliptical slice sampling within Gibbs for
Gompertz model with Poisson sampling error distribution under flat
and normal-inverse gamma priors
}
\details{
This function sample from the posterior distribution of the following model:
\deqn{
 N^\star_t | N_t \sim \mathrm{Poisson}(N_t) \, , \\
 N_t = \exp(Z_t) \, , \\
 Z_t = a + (1 + b) Z_{t - 1} + \varepsilon_t \, , \\
 \varepsilon_t \overset{i.i.d.}{\sim} N(0, \sigma^2) \, .
 }

A different parametrization is used, say:
\deqn{
 \theta_1 = -\frac{a}{b} \, , \, \theta_2 = -\frac{\sigma^2}{b(2 + b)}
 \, ,
 }
which implies
\deqn{
 Z_1, Z_2, ..., Z_T \sim N_T ( \theta_1 1_T, \theta_2 \bar{\Sigma})  \, , \\
 \bar{\Sigma}_{ij} = (1 + b ) ^ {\vert i - j \vert} \, .
}
The priors are:
\deqn{
 b \sim U(-1, 0) \, , \\
 \theta_2 \sim Inv.Gamma(\phi_1, \phi_2) \, , \\
 \theta_1 \vert \theta_2 \sim N(\eta_1, \eta_2 \theta_2) \, .
}
Setting \eqn{\beta = \log(-b / (1 + b))}, it is obtained \eqn{\beta \sim
Logis(0, 1)}, that is \eqn{\beta \vert V \sim N(0, V)} and \eqn{V} is a
logistic Kolmogorov distribution (four times the square of a Kolmogorov
distribution). The following updating scheme is used:
\enumerate{
\item sample \eqn{V \vert \beta} (posterior of a logistic Kolmogorov),
\item for \eqn{t = 1, 2, ..., T}:
sample \eqn{Z_t \vert N^\star_t, Z_{-t}, b, \theta_1, \theta_2}
(using acceptance-rejection),
\item sample \eqn{\beta \vert Z, V, \theta_1, \theta_2} (using elliptical
slice sampling),
\item sample \eqn{\theta_2, \theta_1 \vert Z, \beta} (inverse gamma and
normal distribution respectively).
}
See \link{INSERT REFERENCE} for more details regarding the updating scheme.

\code{starter}: the starting point is generated in the following way:
\itemize{
\item if \code{starter == NULL} then:
\enumerate{
\item set \code{b, theta2, theta1} equal to method of moments
estimates,
\item check if \code{b} is in the support and then get \code{beta},
\item sample \code{Z} using importance sampling resampling.
}
\item \code{else} it must be a named list with names equal to the variables
to be initialized, say \code{starter$theta1, starter$theta2, starter$b,
   starter$Z}.
}

Only one value every \code{thin} values is kept in the chain, so the true
number of complete scans will be \code{nsim * thin + burn}. By default
\code{thin = 1}, that is no thinning.

The current time and the current number of iteration are printed one every
\code{verbose} iterations. Furthermore:
\itemize{
\item if \code{verbose == +-Inf} then there is no printing,
\item if \code{verbose != +-Inf} then at least start and end of simulation
are reported.
}
}
